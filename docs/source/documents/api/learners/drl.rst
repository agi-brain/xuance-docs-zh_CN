DRL Agent
------------

.. toctree::
    :hidden:
    :maxdepth: 1
    :caption: DRL Agent

    DQN <drl/dqn_learner>
    Double DQN <drl/ddqn_learner>
    Dueling DQN <drl/dueldqn_learner>
    PerDQN <drl/perdqn_learner>
    QRDQN <drl/qrdqn_learner>
    C51 <drl/c51_learner>
    DRQN <drl/drqn_learner>
    PG <drl/pg_learner>
    PPG <drl/ppg_learner>
    A2C <drl/a2c_learner>
    PPO-KL <drl/ppokl_learner>
    PPO-Clip <drl/ppoclip_learner>
    SAC <drl/sac_learner>
    SAC-DIS <drl/sacdis_learner>
    DDPG <drl/ddpg_learner>
    TD3 <drl/td3_learner>
    P-DQN <drl/pdqn_learner>
    MP-DQN <drl/mpdqn_learner>
    SP-DQN <drl/spdqn_learner>

* :doc:`Deep Q-Network (DQN) <drl/dqn_learner>`.
* :doc:`Double Deep Q-Network (Double DQN) <drl/ddqn_learner>`.
* :doc:`Dueling Deep Q-Network (Dueling DQN) <drl/dueldqn_learner>`.
* :doc:`DQN with Prioritized Experience Replay (PerDQN) <drl/perdqn_learner>`.
* :doc:`DQN with Quantile Regression (QRDQN) <drl/qrdqn_learner>`.
* :doc:`Categorical 51 DQN (C51) <drl/c51_learner>`.
* :doc:`Deep Recurrent Q-Network (DRQN) <drl/drqn_learner>`.
* :doc:`Policy Gradient (PG) <drl/pg_learner>`.
* :doc:`Phasic Policy Gradient (PPG) <drl/ppg_learner>`.
* :doc:`Advantage Actor Critic (A2C) <drl/a2c_learner>`.
* :doc:`Proximal Policy Optimization with KL Divergence (PPO-KL) <drl/ppokl_learner>`.
* :doc:`Proximal Policy Optimization with Clipped Objective (PPO-Clip) <drl/ppoclip_learner>`.
* :doc:`Soft Actor-Critic (SAC) <drl/sac_learner>`.
* :doc:`Soft Actor-Critic for Discrete Actions (SACDIS) <drl/sacdis_learner>`.
* :doc:`Deep Deterministic Policy Gradient (DDPG) <drl/ddpg_learner>`.
* :doc:`Twin Delayed Deep Deterministic Policy Gradient (TD3) <drl/td3_learner>`.
* :doc:`Parameterised Deep Q-Network (P-DQN) <drl/pdqn_learner>`.
* :doc:`Multi-pass Parameterised Deep Q-Network (MP-DQN) <drl/mpdqn_learner>`.
* :doc:`Split parameterised Deep Q-Network (SP-DQN) <drl/spdqn_learner>`.